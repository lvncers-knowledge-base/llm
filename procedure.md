# 手順

カーパシーの nanoGPT / nanoChat 系はね、
「LLMって中で何してるの？」を実装ベースで理解するのにほんと神教材。

ざっくり全体像からいくね👇

## LLMの基本構造（超ざっくり）

LLMはだいたいこの流れ👇

### 1. トークナイズ

文字 → 数字（token）

例: こんにちは → [1234, 5678, ...]

### 2. Embedding

token → ベクトル

「意味を持った数値」に変換する感じ

### 3. Transformer（本体）

ここが脳みそ 🧠

中身は主に：

Self-Attention

Feed Forward Network

残差接続 + LayerNorm

### 4. Linear + Softmax

次に来そうなtokenの確率を出す

### 5. 損失関数（Loss）

だいたい Cross Entropy Loss

「予測どれくらい外れた？」を数値化

### 6. 最適化（Optimizer）

Adam / AdamW とか

重みをちょっとずつ修正する役

## Transformerって何してるの？

ここが一番モヤっとしやすいとこだけど…

Self-Attentionを一言で言うと

👉 「今の単語を見るとき、過去のどの単語をどれくらい重視するかを計算してる」

Q, K, V とか出てくるけど

本質は「重み付き平均」してるだけ

nanoGPTだとこの辺が100行ちょいで書いてあって、 「あ、意外とシンプルじゃん」ってなると思うよ 😏

## 損失関数と最適化（怖くない版）

### 損失関数

正解tokenが 5

モデルの予測が
5: 0.7, 3: 0.2, 8: 0.1

→ 「まあまあ当たってるね」って低いloss

### 最適化

lossを微分（自動で）

「この重みちょっと減らそ」「ここ増やそ」

を何億回も繰り返す

数学ゴリゴリより
「誤差を減らす方向に重みを動かしてる」
って理解で全然OKだよ 👍

## nanoChat / nanoGPTで勉強するのが良い理由

✅ 最小構成（無駄がない）

✅ PyTorchで全部読める

✅ 実際に学習回せる

✅ 「ブラックボックス感」が消える


### 特におすすめの進め方👇

1. まずREADMEを読む

2. model.pyを上から読む

Embedding

Attention

FFN

3. 「これ何？」ってなったら1個ずつ調べる

4. 小さいデータでtrain回してみる
